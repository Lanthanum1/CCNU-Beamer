\section{其他尝试}

\begin{frame}
    使用 Ollama 并不是最优选择，无论是速度还是显存占用都是不佳的。

    \texttt{OLLAMA\_SCHED\_SPREAD=1} 本来也是不建议开启的，Ollama 只能用单一的 gguf 格式的模型参数进行推理，但是对于 671 B 这么大的模型来说，全部加载在同一张 GPU 上是不太现实的，只能分担在多张GPU上，Ollama 对于一个模型在多卡中推理优化较差，这样做增加了GPU之间通信的成本，即使所有参数全部在GPU中推理，速度也只有 10-40 tokens/s。
\end{frame}

\begin{frame}
    \frametitle{vllm}
    \begin{itemize}
        \item \textbf{vllm}可能是比 Ollama 更优的选择。
              \begin{itemize}
                  \item 原生支持 safetensor 格式。
                  \item 但是 671B fp8 太大了，8卡a100也装不下。
              \end{itemize}

              \bigskip

        \item 考虑使用量化模型以降低显存需求。
              \begin{itemize}
                  \item vllm官网中有一句话，Please note that GGUF support in vLLM is highly experimental and under-optimized at the moment, it might be incompatible with other features.
                  \item 尝试 1.58 bit 动态量化版本报错 (ValueError: GGUF model with architecture deepseek2 is not supported yet.)。
                  \item INT4 W4A16 量化可能是可行的方案 (未尝试)。
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{模型下载问题}
    \subsubsection{模型下载问题}
    \begin{itemize}
        \item 拉取 1.58 bit 动态量化模型时，\texttt{hf mirror} 速度慢且易断连。
        \item \texttt{modelscope} 限速在 15MB/s 左右。
        \item 对于 671B 这么大的模型，可以使用 \texttt{snapshot\_download} 下载部分文件夹。
    \end{itemize}
    % \textbf{使用 \texttt{snapshot\_download} 下载部分文件夹示例 }
    \begin{lstlisting}[language=python]
from huggingface_hub import snapshot_download
snapshot_download(repo_id='unsloth/DeepSeek-R1-GGUF', allow_patterns='DeepSeek-R1-UD-IQ1_S/*', cache_dir='./')

# modelscope

from modelscope.hub.snapshot_download import snapshot_download
model_dir = snapshot_download(repo_id='unsloth/DeepSeek-R1-GGUF', allow_patterns='DeepSeek-R1-UD-IQ1_S/*',cache_dir='./')
\end{lstlisting}
\end{frame}
